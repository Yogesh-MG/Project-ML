 Program 1: Data Summary, Visualization, and Outlier Detection
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = pd.read_csv('titanic.csv')

# Select a numerical column
column = 'Survived'

# Compute statistics
stats = {
    'Mean': data[column].mean(),
    'Median': data[column].median(),
    'Mode': data[column].mode()[0],
    'Std Dev': data[column].std(),
    'Variance': data[column].var(),
    'Range': data[column].max() - data[column].min()
}
print(stats)

# Histogram and Boxplot
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
data[column].hist()
plt.title('Histogram')

plt.subplot(1, 2, 2)
sns.boxplot(x=data[column])
plt.title('Boxplot')
plt.show()

# Outlier Detection using IQR
Q1 = data[column].quantile(0.25)
Q3 = data[column].quantile(0.75)
IQR = Q3 - Q1

outliers = data[(data[column] < Q1 - 1.5 * IQR) | (data[column] > Q3 + 1.5 * IQR)]
print("Outliers:\n", outliers)

# Categorical variable analysis
cat_col = 'Embarked'
category_counts = data[cat_col].value_counts()

# Bar Chart
category_counts.plot(kind='bar', title='Bar Chart of Categories')
plt.xlabel('Category')
plt.ylabel('Frequency')
plt.show()

# Pie Chart (Optional)
category_counts.plot(kind='pie', title='Pie Chart of Categories', autopct='%1.1f%%')
plt.ylabel('')
plt.show()


Program 2: Correlation, Covariance, and Heatmap Visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = pd.read_csv('titanic.csv')

# Select numerical columns
numerical_columns = ['Age', 'Fare']
df = data[numerical_columns].dropna()

# Covariance Matrix
cov_matrix = df.cov()
print("Covariance Matrix:\n", cov_matrix)

# Correlation Matrix
corr_matrix = df.corr()
print("\nCorrelation Matrix:\n", corr_matrix)

# Scatter plot
plt.figure(figsize=(6, 4))
sns.scatterplot(x=df['Age'], y=df['Fare'])
plt.title('Scatter Plot: Age vs Fare')
plt.xlabel('Age')
plt.ylabel('Fare')
plt.show()

# Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix Heatmap')
plt.show()



Program 3: Principal Component Analysis (PCA) on Iris Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create DataFrame
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['Target'] = y

# Scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue='Target', data=pca_df, palette='viridis')
plt.title('PCA on Iris Dataset (2D)')
plt.show()

# Explained Variance
print(f'Explained Variance Ratio: {pca.explained_variance_ratio_}')



Program 4: k-Nearest Neighbors (k-NN) Regular vs Weighted
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from collections import Counter

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Euclidean distance
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2)**2))

# k-NN function
def knn(X_train, y_train, X_test, k=3, weighted=False):
    y_pred = []
    for test_point in X_test:
        distances = [euclidean_distance(test_point, train_point) for train_point in X_train]
        k_indices = np.argsort(distances)[:k]
        k_nearest_labels = [y_train[i] for i in k_indices]
        
        if weighted:
            weights = [1 / (distances[i]**2) if distances[i]!=0 else 1e10 for i in k_indices]
            class_votes = {}
            for label, weight in zip(k_nearest_labels, weights):
                class_votes[label] = class_votes.get(label, 0) + weight
            y_pred.append(max(class_votes, key=class_votes.get))
        else:
            y_pred.append(Counter(k_nearest_labels).most_common(1)[0][0])
    return np.array(y_pred)

# Test models for different k values
k_values = [1, 3, 5]
results = {'k': [], 'Accuracy (Regular k-NN)': [], 'F1-Score (Regular k-NN)': [], 
           'Accuracy (Weighted k-NN)': [], 'F1-Score (Weighted k-NN)': []}

for k in k_values:
    # Regular k-NN
    y_pred_knn = knn(X_train, y_train, X_test, k=k, weighted=False)
    acc_knn = accuracy_score(y_test, y_pred_knn)
    f1_knn = f1_score(y_test, y_pred_knn, average='macro')

    # Weighted k-NN
    y_pred_wknn = knn(X_train, y_train, X_test, k=k, weighted=True)
    acc_wknn = accuracy_score(y_test, y_pred_wknn)
    f1_wknn = f1_score(y_test, y_pred_wknn, average='macro')

    # Store results
    results['k'].append(k)
    results['Accuracy (Regular k-NN)'].append(acc_knn)
    results['F1-Score (Regular k-NN)'].append(f1_knn)
    results['Accuracy (Weighted k-NN)'].append(acc_wknn)
    results['F1-Score (Weighted k-NN)'].append(f1_wknn)

# Results DataFrame
results_df = pd.DataFrame(results)
print(results_df)

# Accuracy and F1-Score Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Accuracy
axes[0].plot(results_df['k'], results_df['Accuracy (Regular k-NN)'], label='Regular k-NN', marker='o')
axes[0].plot(results_df['k'], results_df['Accuracy (Weighted k-NN)'], label='Weighted k-NN', marker='o')
axes[0].set_title('Accuracy Comparison')
axes[0].set_xlabel('k')
axes[0].set_ylabel('Accuracy')
axes[0].legend()

# F1-Score
axes[1].plot(results_df['k'], results_df['F1-Score (Regular k-NN)'], label='Regular k-NN', marker='o')
axes[1].plot(results_df['k'], results_df['F1-Score (Weighted k-NN)'], label='Weighted k-NN', marker='o')
axes[1].set_title('F1-Score Comparison')
axes[1].set_xlabel('k')
axes[1].set_ylabel('F1-Score (Macro)')
axes[1].legend()

plt.tight_layout()
plt.show()



Program 5: Locally Weighted Regression (LWR)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load Iris dataset
iris = load_iris()
data = pd.DataFrame(iris.data, columns=iris.feature_names)

# Predict 'sepal length' based on 'sepal width'
X = data[['sepal width (cm)']]
y = data['sepal length (cm)']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Locally Weighted Regression
def locally_weighted_regression(X_train, y_train, X_test, tau=1.0):
    m = X_train.shape[0]
    y_pred = np.zeros(X_test.shape[0])

    for i in range(X_test.shape[0]):
        weights = np.exp(-np.sum((X_train - X_test.iloc[i])**2, axis=1) / (2 * tau**2))
        X_weighted = X_train * weights[:, np.newaxis]
        y_weighted = y_train * weights
        
        # Weighted least squares solution
        theta = np.linalg.inv(X_weighted.T @ X_weighted) @ (X_weighted.T @ y_weighted)
        y_pred[i] = np.dot(X_test.iloc[i], theta)
    return y_pred

# Perform LWR
y_pred = locally_weighted_regression(X_train, y_train, X_test, tau=1.0)

# Plot Results
plt.scatter(X_train, y_train, color='blue', label='Training Data')
plt.scatter(X_test, y_test, color='red', label='Test Data')
plt.plot(X_test, y_pred, color='green', label='Locally Weighted Regression')
plt.xlabel('Sepal Width (cm)')
plt.ylabel('Sepal Length (cm)')
plt.title('Locally Weighted Regression on Iris Dataset')
plt.legend()
plt.show()
